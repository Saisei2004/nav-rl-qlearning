強化学習 (Q学習) によるロボットナビゲーション
このプロジェクトは、Pygameで構築された2Dシミュレーション環境内で、差動駆動ロボットが指定されたゴール（位置と角度）に到達するための強化学習（Q学習）エージェントを実装・訓練するものです。静的・動的障害物や複雑な迷路など、様々な難易度の環境が用意されています。学習プロセスは並列化されており、ハイパーパラメータのチューニングやTensorBoardによる学習状況の可視化もサポートしています。

✨ 特徴
Pygameベースのシミュレーター (game_simulator.py):

差動駆動ロボットのための2Dシミュレーション環境。

環境認識のためのLiDAR（ライダー）センサーのシミュレーション。

複数の難易度レベル（"Step"）による多様なシナリオ：

Step 0-2: 基本的なゴール到達タスク。

Step 3-4: ランダムな数の静的障害物がある環境。

Step 5-6: 壁と静的障害物からなる迷路状の環境。

Step 7-8: 複雑な迷路に動的障害物が加わった環境。

多様な動的障害物：

SlowBouncingObstacle: 一定速度で移動し、壁で反射する。

AppearingObstacle: ランダムな間隔で出現・消滅する。

CircularObstacle: 円形の軌道を描いて移動する。

BlinkingObstacle: 開閉するドアのように機能する。

強化学習環境 (rl_env.py):

シミュレーターをGymライクなインターフェース（reset, step）でラップ。

状態表現: ゴールまでの距離、ゴールへの角度差、正規化されたLiDARの距離データをベクトルとして表現。

カスタマイズ可能な報酬関数: さまざまな戦略を試すために、報酬関数を容易に変更可能。

Q学習エージェントと学習プロセス (train_rl.py):

離散化された状態行動空間から方策を学習するQ学習エージェント。

並列学習: Pythonのmultiprocessingモジュールを使用し、複数のCPUコアで最適なハイパーパラメータのグリッドサーチを実行。

TensorBoard連携: エピソードごとの報酬や成功率などの主要なメトリクスを記録し、TensorBoardで可視化。

ハイパーパラメータ調整: grid_search関数により、報酬関数のパラメータの様々な組み合わせを体系的にテスト。

📁 ファイル構成
game_simulator.py: シミュレーション環境のコアロジック。ロボット、障害物、物理演算、描画処理などが含まれます。このファイルを直接実行することで、テスト環境でロボットを手動操作できます。

rl_env.py: シミュレーターを強化学習エージェントに適した環境（GameEnvクラス）に変換するラッパー。

train_rl.py: Q学習エージェント（QAgent）、並列化学習とハイパーパラメータ検索のロジック（grid_search, worker）、TensorBoardロギングの設定を実装。強化学習を開始するためのメインファイルです。

🚀 導入方法
リポジトリをクローンします。

git clone https://github.com/Saisei2004/nav-rl-qlearning.git
cd nav-rl-qlearning

必要なPythonライブラリをインストールします。仮想環境の使用を推奨します。

pip install -r requirements.txt
```requirements.txt`ファイルがない場合は、以下のライブラリを手動でインストールしてください。
```bash
pip install pygame numpy torch tensorboard

使い方
1. 強化学習の実行
Q学習エージェントの学習を開始するには、train_rl.pyスクリプトを実行します。これにより、スクリプト内で定義されたハイパーパラメータのグリッドサーチが開始されます。

python train_rl.py

スクリプトは、さまざまなハイパーパラメータの組み合わせを試し、並列で学習セッションを実行し、各設定の成功率を出力します。Ctrl+Cでいつでもプロセスを中断でき、そこまでの結果の集計が表示されます。

2. TensorBoardでの監視
学習スクリプトは、TensorBoard用のデータを自動的にログとして記録します。TensorBoardを起動することで、学習の進捗をリアルタイムで視覚化できます。

グリッドサーチ完了後、スクリプトは自動的にTensorBoardを起動し、ブラウザで開こうとします。

手動で起動する場合は、プロジェクトのルートディレクトリで以下のコマンドを実行します。

tensorboard --logdir=runs

ウェブブラウザで http://localhost:6006 を開きます。各ワーカープロセスのReward/Episode（エピソード報酬）やSuccessRate/Episode（成功率）などのメトリクスを確認できます。

3. シミュレーションの手動実行
強化学習エージェントなしでシミュレーションを実行し、環境の感触を確かめることができます。game_simulator.pyを開き、if __name__ == "__main__":ブロック内のmodeを、利用可能な"Step"モード（例: "Step_8"）のいずれかに変更します。

# game_simulator.py 内
if __name__ == "__main__":
    # モードを変更して様々な環境をテスト
    game = Game(obstacle_count=10, mode="Step_8")
    game.run()

その後、ファイルを実行します。

python game_simulator.py

以下のキーを使用して、ロボットの車輪を手動で制御します。

左車輪: 2 (高速前進), w (低速前進), s (低速後退), x (高速後退)

右車輪: 1 (高速前進), q (低速前進), a (低速後退), z (高速後退)

🔧 カスタマイズ
学習パラメータ: train_rl.pyのgrid_search関数で、エピソード数、各エピソードの最大ステップ数、並列プロセス数などを調整できます。

ハイパーパラメータ探索空間: grid_search内のパラメータリスト（例: angle_bonus_list, step_penalty_list）を変更して、さまざまな報酬構造を探索できます。

報酬関数: train_rl.py内のimproved_reward関数は高度にカスタマイズ可能です。ロジックを追加、削除、変更して、エージェントの行動を誘導できます。

行動空間: train_rl.pyの先頭にあるACTION_SETで、可能な行動（車輪の速度）のセットが定義されています。これを変更して、より細かい、あるいは異なる種類の動きを許可することができます。

シミュレーション環境: game_simulator.pyで新しい"Step"モードを作成し、エージェントのための新しい課題を設計できます。

📜 ライセンス
このプロジェクトはオープンソースです。詳細はLICENSEファイルを参照してください。
