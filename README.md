強化学習（Q学習）によるロボットナビゲーション
このプロジェクトは、Q学習エージェントを訓練し、差動駆動ロボットが2D環境で指定されたゴールに到達することを目指すものです。シミュレーションはPygameで構築されており、静的・動的な障害物や複雑な迷路など、様々な難易度のレベルが用意されています。訓練プロセスは並列化されており、ハイパーパラメータのチューニングや訓練状況を監視するためのTensorBoard連携機能も含まれています。

特徴
Pygameベースのシミュレータ (game_simulator.py):

差動駆動ロボットのための2Dシミュレーション環境
環境認識のためのLiDARセンサーシミュレーション
複数の難易度レベル（"Step"）による多様なシナリオ:
Step 0-2: 基本的なゴール到達タスク
Step 3-4: ランダムな数の静的障害物がある環境
Step 5-6: 壁と静的障害物のある迷路のような環境
Step 7-8: 動的障害物のある複雑な迷路
多様な動的障害物:
SlowBouncingObstacle: 一定速度で移動し、壁で跳ね返る
AppearingObstacle: ランダムな間隔で出現・消滅する
CircularObstacle: 円形の軌道で移動する
BlinkingObstacle: 開閉するドアとして機能する
強化学習環境 (rl_env.py):

シミュレータをラップし、Gymライクなインターフェース（reset, step）を提供
状態表現: 状態は、ゴールまでの距離、ゴールへの角度差、正規化されたLiDARの距離データを含むベクトルで表現されます
カスタマイズ可能な報酬: 報酬関数は、様々な戦略を試すために簡単に変更できます
Q学習エージェントと訓練 (train_rl.py):

離散化された状態行動空間から方策を学習するQ学習エージェント
並列学習: Pythonのmultiprocessingモジュールを使用して、複数のCPUコアでハイパーパラメータのグリッドサーチを実行
TensorBoard連携: エピソード報酬や成功率などの主要なメトリクスを記録し、TensorBoardで可視化
ハイパーパラメータチューニング: grid_search関数が実装されており、報酬関数のパラメータの様々な組み合わせを体系的にテストします
ファイル概要
game_simulator.py: ロボット、障害物、物理演算、描画など、シミュレーション環境のコアロジックが含まれています。このファイルを直接実行すると、テスト環境でロボットを手動で操作できます。
rl_env.py: シミュレータをエージェントに適した強化学習環境にラップするGameEnvクラスを定義しています。
train_rl.py: Q学習エージェント（QAgent）、並列化された訓練とハイパーパラメータ検索のロジック（grid_search, worker）、TensorBoardのロギング設定を実装しています。RL訓練を開始するためのメインファイルです。
インストール
リポジトリをクローンします:

Bash

git clone https://github.com/Saisei2004/nav-rl-qlearning.git
cd nav-rl-qlearning
必要なPythonライブラリをインストールします。仮想環境の使用を推奨します。
requirements.txtが利用できない場合は、手動で依存関係をインストールしてください:

Bash

pip install pygame numpy torch tensorboard
使い方
1. RLトレーニングの実行
Q学習エージェントの訓練を開始するには、train_rl.pyスクリプトを実行します。これにより、スクリプト内で定義されたハイパーパラメータのグリッドサーチが開始されます。

Bash

python train_rl.py
スクリプトは、異なるハイパーパラメータのセットを順番に試し、並列で訓練セッションを実行し、各セットの成功率を出力します。Ctrl+Cでいつでもプロセスを停止でき、その時点までの結果の要約が表示されます。

2. TensorBoardでの監視
訓練スクリプトはTensorBoard用のデータを自動的に記録します。これを起動することで、訓練の進捗をリアルタイムで視覚化できます。

グリッドサーチ完了後、スクリプトは自動的にTensorBoardを起動し、ブラウザで開こうとします。
または、プロジェクトのルートディレクトリで以下のコマンドを実行して手動で起動することもできます:
Bash

tensorboard --logdir=runs
ウェブブラウザで http://localhost:6006 にアクセスしてダッシュボードを表示します。各ワーカープロセスのReward/EpisodeやSuccessRate/Episodeなどのメトリクスを確認できます。
3. シミュレーションの手動実行
環境の動作を確認するために、RLエージェントなしでシミュレータを実行することができます。game_simulator.pyを開き、if __name__ == "__main__":ブロック内のmodeを、利用可能な "Step" モード（例: "Step_8"）のいずれかに変更します。

Python

# game_simulator.py内
if __name__ == "__main__":
    # モードを変更して異なる環境をテスト
    game = Game(obstacle_count=10, mode="Step_8")
    game.run()
その後、ファイルを実行します:

Bash

python game_simulator.py
以下のキーを使用して、ロボットの車輪を手動で制御します:

左車輪: 2 (高速前進), w (低速前進), s (低速後退), x (高速後退)
右車輪: 1 (高速前進), q (低速前進), a (低速後退), z (高速後退)
カスタマイズ
訓練パラメータ: train_rl.pyのgrid_search関数で、エピソード数、エピソードあたりのステップ数、並列プロセス数を調整できます。
ハイパーパラメータ探索空間: grid_search内のパラメータリスト（例: angle_bonus_list, step_penalty_list）を変更して、異なる報酬構造を探索できます。
報酬関数: train_rl.pyのimproved_reward関数は高度にカスタマイズ可能です。エージェントの行動を導くために、ロジックを追加、削除、または変更できます。
行動空間: train_rl.pyの冒頭にあるACTION_SETで、可能な行動（車輪の速度）のセットが定義されています。これを変更して、より細かい、または異なる種類の動きを許可することができます。
シミュレーション環境: game_simulator.pyで新しい "Step" モードを作成し、エージェントのための新しい挑戦を設計することができます。
ライセンス
このプロジェクトはオープンソースです。詳細はLICENSEファイルを参照してください。
